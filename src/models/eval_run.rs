/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * OpenAPI spec version: 2.3.0
 *
 * Generated by: https://github.com/swagger-api/swagger-codegen.git
 */

/// EvalRun : A schema representing an evaluation run.

#[allow(unused_imports)]
use serde_json::Value;

#[derive(Debug, Serialize, Deserialize)]
pub struct EvalRun {
  /// Unix timestamp (in seconds) when the evaluation run was created.
  #[serde(rename = "created_at")]
  created_at: i32,
  /// Information about the run's data source.
  #[serde(rename = "data_source")]
  data_source: Value,
  #[serde(rename = "error")]
  error: crate::models::EvalApiError,
  /// The identifier of the associated evaluation.
  #[serde(rename = "eval_id")]
  eval_id: String,
  /// Unique identifier for the evaluation run.
  #[serde(rename = "id")]
  id: String,
  #[serde(rename = "metadata")]
  metadata: crate::models::Metadata,
  /// The model that is evaluated, if applicable.
  #[serde(rename = "model")]
  model: String,
  /// The name of the evaluation run.
  #[serde(rename = "name")]
  name: String,
  /// The type of the object. Always \"eval.run\".
  #[serde(rename = "object")]
  object: String,
  /// Usage statistics for each model during the evaluation run.
  #[serde(rename = "per_model_usage")]
  per_model_usage: Vec<crate::models::EvalRunPerModelUsage>,
  /// Results per testing criteria applied during the evaluation run.
  #[serde(rename = "per_testing_criteria_results")]
  per_testing_criteria_results: Vec<crate::models::EvalRunPerTestingCriteriaResults>,
  /// The URL to the rendered evaluation run report on the UI dashboard.
  #[serde(rename = "report_url")]
  report_url: String,
  #[serde(rename = "result_counts")]
  result_counts: crate::models::EvalRunResultCounts,
  /// The status of the evaluation run.
  #[serde(rename = "status")]
  status: String
}
